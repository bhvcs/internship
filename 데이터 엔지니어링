회사가 금융회사다 보니 다양한 금융 지표가 많이 필요하다.

그래서 데이터를 수집하고 사내 데이터베이스에 적재하는 일을 많이 했다. 
어떤 데이터가 필요하다 하면, 그 데이터가 있는 곳에서 어떻게 수집할 수 있을지 고민한다. 첫번째로는 내가 사용할 수 있는 외부 라이브러리나 공개된 외부 API가 있는지 확인했다.
이런 상황에서는 그냥 사용하거나, 혹은 수집하고자 하는 서비스에서 발급 받는 토큰으로 손쉽게 데이터를 들고 올 수 있었다.
이와 같이 공개된 라이브러리가 없는 곳에서 데이터를 가져와야 하는 경우도 많았는데, 이런 경우에 파이썬을 이용한 웹 스크래핑을 사용해서 데이터를 수집했다. 

데이터를 수집했으면 이를 내가 원하는 형태로 가공한 후, 데이터베이스에 적재를 한다. 보통 데이터의 형식을 맞춘다거나, 수집한 데이터 자체가 계산을 위한 low 데이터라 계산을 수행하고 저장하는 경우가 대부분이었다.
이를 위해 파이썬의 데이터프레임이나 시리즈 객체에 대한 공부를 많이 했었다. 데이터프레임 객체가 데이터베이스와 호환이 잘 되어서 데이터 적재 및 받아오는게 편하기 때문이다. 데이터베이스와의 소통을 위해 사용한 파이썬
라이브러리가 있는데 sqlAlchemy였다. 해당 라이브러리를 이용해서 파이썬 스크립트로, 설정된 데이터베이스에 손쉽게 명령을 할 수 있었다.

내가 하는 작업의 최종 목표는 데이터 수집 작업을 주기적으로 수행할 수 있게, 배치 프로그램으로 만들어서, 주기적으로 필요한 데이터가 데이터베이스에 자동적으로 적재되게 끔 하는 것이었다.
코드 실행의 자동화를 위해 jenkins를 사용했다고 하였지만 그 부분은 내가 직접 설정할 순 없었고, 배치 작업 프로그램만 만들어주었다.

주기적으로 데이터를 db에 적재하는 프로그램이지만, 실행할 때 기입하는 인자에 맞추어 다르게 동작하도록 만들었다. 예를 들면, 과거에 수집을 하지 않았던 데이터라 이전 데이터들이 비어있는 경우,
내가 설정한 기간 동안의 데이터들을 적재할 수 있도록 만들었다.

데이터 스크래핑 할 때, 공개된 라이브러리가 없는 경우에 어떻게 데이터를 가져와야 할지 고민을 많이 했었다. 그때 사수님께서 자신이 스크래핑할 때 사용하는 팁 같은 것들을 많이 알려주셨다.
특별한 건 아니지만 웹 api에 대한 근본적인 접근 방식으로 수집을 하는데, 이 덕에 api에 대한 더 많은 이해를 할 수 있었다.
방법은 개발자 모드를 키고 -> 내가 찾고자 하는 데이터가 있는 페이지를 들어간다 -> network 탭에서 호출되는 api를 확인할 수 있다 -> 파이썬으로 해당 api를 호출한다.
이렇게 데이터를 수집할 때는, 내가 호출한 api에서 어떤 응답이 오는지 분석하는 게 중요하다. 근데 웹 api다 보니 응답량이 대체로 많고 편집기( 내 경우엔 파이참)에서는 해당 응답을 분석하기에는 불편했다.
그래서 postman을 주로 사용해서 응답을 받아봤다. postman을 사용하면 좋은 점이, 어떤 사이트는 쿠키 설정을 해야하는 경우가 많았는데, postman에서 이런 부분의 설정을 확인하기가 좋았다.

수행 업무에는 db의 데이터로 유용한 엑셀 파일 생성하는 것도 있었다.
db의 필요한 데이터들을 데이터프레임 객체로 가지고 와, 유의미하게 가공하고, 계산된 데이터들을 지정된 형식에 따라 엑셀 파일을 채우는 것이다. 해당 엑셀 파일은 IR 자료로 사용되었고, 해당 파일도
배치작업으로 만들어 매주 엑셀 파일을 만들도록 하였다.
